{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone scholar repo, set up new environment, install packages\n",
    "git clone git@github.com:dallascard/scholar.git"
   ]
  },
  {
   "source": [
    "Open file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "with open(os.path.join( 'df_train.json')) as f:\n",
    "    lines = f.readlines()\n",
    "first_doc = json.loads(lines[0])\n",
    "for key, value in first_doc.items():\n",
    "    print(key, ':', value)"
   ]
  },
  {
   "source": [
    "Preprocessing\n",
    "--min doc count: minimum times a word has to appear across documents to be included\n",
    "-- label: metadata, see in paper\n",
    "-- keep alphanum True: keep words with numbers and letters mixed in it\n",
    "-- test ft_test.json : precprocessing the test and train sets simulatiosuly"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = 'preprocess_data.py'\n",
    "args = 'df_train.json our_data --min-doc-count 4 --label type, star_rating, brand, usefulness --keep-alphanum True --strip-html True --test df_test.json'\n",
    "print(\"python\", script, args)\n",
    "preprocess_data.main(args.split())"
   ]
  },
  {
   "source": [
    "We can look at the vocab"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabualry\n",
    "with open(os.path.join( 'our_data', 'train.vocab.json')) as f:\n",
    "    vocab = json.load(f)\n",
    "print(\"First few words in the vocbulary:\")\n",
    "print(vocab[:20] + ['...'])"
   ]
  },
  {
   "source": [
    "Running model without meta-data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_scholar\n",
    "script = 'run_scholar.py'\n",
    "args = 'our_data/ -k 9 --epochs 100 --dev-folds 10 --seed 70 --test-prefix test'\n",
    "print(\"python\", script, args)\n",
    "model = run_scholar.main(args.split())"
   ]
  },
  {
   "source": [
    "Internal topic coherence was calculated in the terminal using the following code (SCHOLAR):\n",
    "python compute_npmi.py output/topics.txt our_data/test.npz our_data/train.vocab.json\n",
    "\n",
    "Check out most common words and their background frequencies\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# load the background log-frequencies\n",
    "bg = np.load('output/bg.npz')['bg']\n",
    "\n",
    "# load the vocabualry\n",
    "with open('output/vocab.json') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# sort terms by log-frequency\n",
    "order = np.argsort(bg)\n",
    "\n",
    "# print the most common words \n",
    "for i in range(1, 25):\n",
    "    index = order[-i]\n",
    "    print(vocab[index], np.exp(bg[index]))"
   ]
  },
  {
   "source": [
    "Look at topics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_scholar import print_top_words\n",
    "\n",
    "# load the stored (K x V) topic matrix (stored in a compressed numpy format)\n",
    "beta = np.load(os.path.join('output', 'beta.npz'))['beta']\n",
    "print_top_words(beta, vocab, n_pos=15, n_neg=5);"
   ]
  },
  {
   "source": [
    "Run model with metadata"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_scholar\n",
    "script = 'run_scholar.py'\n",
    "args = 'our_data/ -k 9 --epochs 100 --dev-folds 10 --seed 42 --labels type --topic-covars star_rating --interaction star_rating --test-prefix test'\n",
    "print(\"python\", script, args)\n",
    "run_scholar.main(args.split())"
   ]
  },
  {
   "source": [
    "Print words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.load('output/beta.npz')['beta']\n",
    "print_top_words(beta, vocab, n_pos=15, n_neg=5);"
   ]
  },
  {
   "source": [
    "Vectors learned for each covariate level (star rating)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_covars = np.load(os.path.join('output', 'beta_c.npz'))\n",
    "weights = topic_covars['beta']\n",
    "names = topic_covars['names']\n",
    "print_top_words(weights, vocab, topic_names=names, n_pos=10, n_neg=5);"
   ]
  },
  {
   "source": [
    "Visualize things, forst topic distribtutions in documents "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the matrix with topic proportions for each document (note that this excludes those in the dev set).\n",
    "npz = np.load(os.path.join('output', 'theta.train.npz')) \n",
    "ids = npz['ids']\n",
    "theta = npz['theta']\n",
    "n_docs, n_topics = theta.shape\n",
    "\n",
    "index = 45\n",
    "# plot the proportion of each topic in the first document\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(n_topics), theta[index, :])\n",
    "\n",
    "# find the original line corresponding to this document, and display the text\n",
    "print(ids[index])\n",
    "for line in lines:\n",
    "    doc = json.loads(line)\n",
    "    if doc['id'] == ids[index]:\n",
    "        print(doc['text'])\n",
    "        break"
   ]
  },
  {
   "source": [
    "Visualize topics over variables, starting with types\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load the type variable for all the documents\n",
    "types_df = pd.read_csv(os.path.join( 'our_data', 'train.type.csv'), header=0, index_col=0)\n",
    "types = types_df.columns\n",
    "\n",
    "# pull out a subset corresponding to the ids from above\n",
    "train_subset = types_df.loc[ids]\n",
    "n_docs, n_types = train_subset.shape\n",
    "\n",
    "# plot the average type-topic proportions\n",
    "fig, ax = plt.subplots()\n",
    "lefts = np.zeros(n_types)\n",
    "for k in range(n_topics):\n",
    "    vals = []\n",
    "    for typer in types:\n",
    "        vals.append(np.mean(theta[train_subset[typer] == 1, k]))\n",
    "\n",
    "    ax.barh(range(n_types), vals, left=lefts)\n",
    "    lefts += np.array(vals)\n",
    "    \n",
    "ax.set_yticks(range(n_types))\n",
    "ax.set_yticklabels(types)\n",
    "plt.show();"
   ]
  },
  {
   "source": [
    "Plot topics across star rating\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the star rating variable for all the documents\n",
    "stars_df = pd.read_csv(os.path.join( 'our_data', 'train.star_rating.csv'), header=0, index_col=0)\n",
    "stars = stars_df.columns\n",
    "\n",
    "# pull out a subset corresponding to the ids from above\n",
    "train_subset = stars_df.loc[ids]\n",
    "n_docs, n_stars = train_subset.shape\n",
    "\n",
    "# plot the average star rating-topic proportions\n",
    "fig, ax = plt.subplots()\n",
    "lefts = np.zeros(n_stars)\n",
    "for k in range(n_topics):\n",
    "    vals = []\n",
    "    for star in stars:\n",
    "        vals.append(np.mean(theta[train_subset[star] == 1, k]))\n",
    "\n",
    "    ax.barh(range(n_stars), vals, left=lefts)\n",
    "    lefts += np.array(vals)\n",
    "    \n",
    "ax.set_yticks(range(n_stars))\n",
    "ax.set_yticklabels(stars)\n",
    "plt.show();"
   ]
  },
  {
   "source": [
    "Topics over usefulness"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the usefulness variable for all the documents\n",
    "usefulness_df = pd.read_csv(os.path.join( 'our_data', 'train.usefulness.csv'), header=0, index_col=0)\n",
    "usefulness = usefulness_df.columns\n",
    "\n",
    "# pull out a subset corresponding to the ids from above\n",
    "train_subset = usefulness_df.loc[ids]\n",
    "n_docs, n_usefulness = train_subset.shape\n",
    "\n",
    "# plot the average usefulness-topic proportions\n",
    "fig, ax = plt.subplots()\n",
    "lefts = np.zeros(n_usefulness)\n",
    "for k in range(n_topics):\n",
    "    vals = []\n",
    "    for useful in usefulness:\n",
    "        vals.append(np.mean(theta[train_subset[useful] == 1, k]))\n",
    "\n",
    "    ax.barh(range(n_usefulness), vals, left=lefts)\n",
    "    lefts += np.array(vals)\n",
    "    \n",
    "ax.set_yticks(range(n_usefulness))\n",
    "ax.set_yticklabels(usefulness)\n",
    "plt.show();"
   ]
  },
  {
   "source": [
    "Plot topics across brands"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the brand variable for all the documents\n",
    "brands_df = pd.read_csv(os.path.join( 'our_data', 'train.brand.csv'), header=0, index_col=0)\n",
    "brands = brands_df.columns\n",
    "\n",
    "# pull out a subset corresponding to the ids from above\n",
    "train_subset = brands_df.loc[ids]\n",
    "n_docs, n_brands = train_subset.shape\n",
    "\n",
    "# plot the average brand-topic proportions\n",
    "fig, ax = plt.subplots()\n",
    "lefts = np.zeros(n_brands)\n",
    "for k in range(n_topics):\n",
    "    vals = []\n",
    "    for brand in brands:\n",
    "        vals.append(np.mean(theta[train_subset[brand] == 1, k]))\n",
    "\n",
    "    ax.barh(range(n_brands), vals, left=lefts)\n",
    "    lefts += np.array(vals)\n",
    "    \n",
    "ax.set_yticks(range(n_brands))\n",
    "ax.set_yticklabels(brands)\n",
    "plt.show();"
   ]
  },
  {
   "source": [
    "Look at which topics predict which type"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('output/topics_to_labels.npz')\n",
    "probs = npz['probs']\n",
    "label_names = npz['label']\n",
    "n_topics, n_labels = probs.shape\n",
    "print(\"Labels:\", ' '.join([name for name in label_names]))\n",
    "for k in range(n_topics):\n",
    "    output = str(k) + ': '\n",
    "    for i in range(n_labels):\n",
    "        output += '%.4f ' % probs[k, i]\n",
    "    print(output)"
   ]
  }
 ]
}
